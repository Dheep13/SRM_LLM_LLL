{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV4WiTV5KuP9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24317,
     "status": "ok",
     "timestamp": 1761543461887,
     "user": {
      "displayName": "Deepan Shanmugam",
      "userId": "10513139089640654961"
     },
     "user_tz": -330
    },
    "id": "oms87dnUK3Qm",
    "outputId": "d9072c55-88ef-4ad0-8f73-3248af2f9990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_dir = '/content/drive/MyDrive/LawBot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761543473943,
     "user": {
      "displayName": "Deepan Shanmugam",
      "userId": "10513139089640654961"
     },
     "user_tz": -330
    },
    "id": "38rpLXCoKuP-",
    "outputId": "ca80c498-5e9d-4001-90b3-3d60c6fdfe0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up directory structure...\n",
      "Using base directory: /content/LawBot\n",
      "\n",
      "Creating directory structure...\n",
      "  ✅ /content/LawBot\n",
      "  ✅ /content/LawBot/datasets\n",
      "  ✅ /content/LawBot/data\n",
      "  ✅ /content/LawBot/data/processed\n",
      "  ✅ /content/LawBot/models\n",
      "  ✅ /content/LawBot/models/adapters\n",
      "  ✅ /content/LawBot/vectorstore\n",
      "  ✅ /content/LawBot/vectorstore/faiss_index\n",
      "\n",
      "Checking for required files:\n",
      "  ❌ constitution_qa.json - MISSING!\n",
      "  ❌ crpc_qa.json - MISSING!\n",
      "  ❌ ipc_qa.json - MISSING!\n",
      "\n",
      "❌ Some files are missing. Please upload them to: /content/LawBot/datasets/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Setup directory structure\n",
    "print(\"Setting up directory structure...\")\n",
    "\n",
    "# Define base paths - using hardcoded Colab path for simplicity\n",
    "base_dir = '/content/LawBot'\n",
    "print(\"Using base directory:\", base_dir)\n",
    "\n",
    "# Create all necessary directories\n",
    "dirs_to_create = [\n",
    "    base_dir,\n",
    "    f'{base_dir}/datasets',\n",
    "    f'{base_dir}/data',\n",
    "    f'{base_dir}/data/processed',\n",
    "    f'{base_dir}/models',\n",
    "    f'{base_dir}/models/adapters',\n",
    "    f'{base_dir}/vectorstore',\n",
    "    f'{base_dir}/vectorstore/faiss_index'\n",
    "]\n",
    "\n",
    "print(\"\\nCreating directory structure...\")\n",
    "for dir_path in dirs_to_create:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"  ✅ {dir_path}\")\n",
    "\n",
    "datasets_path = f'{base_dir}/datasets'\n",
    "\n",
    "# Verify required files exist\n",
    "required_files = ['constitution_qa.json', 'crpc_qa.json', 'ipc_qa.json']\n",
    "print(\"\\nChecking for required files:\")\n",
    "all_present = True\n",
    "for file in required_files:\n",
    "    file_path = f'{datasets_path}/{file}'\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
    "        print(f\"  ✅ {file} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} - MISSING!\")\n",
    "        all_present = False\n",
    "\n",
    "if not all_present:\n",
    "    print(f\"\\n❌ Some files are missing. Please upload them to: {datasets_path}/\")\n",
    "else:\n",
    "    print(f\"\\n✅ All files present. Ready to proceed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6x_3dHVKuQA"
   },
   "source": [
    "# Phase 1: Dataset Preparation for LawBot\n",
    "\n",
    "## Objectives:\n",
    "1. Load and merge constitution_qa.json, crpc_qa.json, and ipc_qa.json\n",
    "2. Transform to instruction format: {instruction, output, source}\n",
    "3. Clean and deduplicate data\n",
    "4. Split 80:20 into train/validation sets\n",
    "5. Generate preprocessing report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqJ33-aDKuQB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9yZbY_JKuQB"
   },
   "source": [
    "## Step 1: Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xa9N5ssHKuQC"
   },
   "outputs": [],
   "source": [
    "# Load all three datasets\n",
    "# Path is determined by the setup cell above\n",
    "print(f\"Loading datasets from: {datasets_path}/\")\n",
    "\n",
    "with open(f'{datasets_path}/constitution_qa.json', 'r', encoding='utf-8') as f:\n",
    "    constitution_data = json.load(f)\n",
    "\n",
    "with open(f'{datasets_path}/crpc_qa.json', 'r', encoding='utf-8') as f:\n",
    "    crpc_data = json.load(f)\n",
    "\n",
    "with open(f'{datasets_path}/ipc_qa.json', 'r', encoding='utf-8') as f:\n",
    "    ipc_data = json.load(f)\n",
    "\n",
    "print(f\"Constitution Q&A: {len(constitution_data)} pairs\")\n",
    "print(f\"CrPC Q&A: {len(crpc_data)} pairs\")\n",
    "print(f\"IPC Q&A: {len(ipc_data)} pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZE4KaGAKuQC"
   },
   "source": [
    "## Step 2: Transform to Instruction Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yhvQQoFKuQC"
   },
   "outputs": [],
   "source": [
    "def transform_to_instruction_format(data, source_name):\n",
    "    \"\"\"Transform data from {question, answer} to {instruction, output, source}\"\"\"\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        formatted_data.append({\n",
    "            \"instruction\": item[\"question\"],\n",
    "            \"output\": item[\"answer\"],\n",
    "            \"source\": source_name\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "# Transform each dataset\n",
    "constitution_formatted = transform_to_instruction_format(constitution_data, \"Constitution\")\n",
    "crpc_formatted = transform_to_instruction_format(crpc_data, \"CrPC\")\n",
    "ipc_formatted = transform_to_instruction_format(ipc_data, \"IPC\")\n",
    "\n",
    "print(f\"Constitution formatted: {len(constitution_formatted)}\")\n",
    "print(f\"CrPC formatted: {len(crpc_formatted)}\")\n",
    "print(f\"IPC formatted: {len(ipc_formatted)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pay-5ARfKuQC"
   },
   "source": [
    "## Step 3: Merge Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73RCHhniKuQD"
   },
   "outputs": [],
   "source": [
    "# Combine all datasets\n",
    "combined_data = constitution_formatted + crpc_formatted + ipc_formatted\n",
    "print(f\"Total combined data: {len(combined_data)} pairs\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data:\")\n",
    "print(json.dumps(combined_data[0], indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaM0o9U4KuQD"
   },
   "source": [
    "## Step 4: Clean and Deduplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0w8Ce3LKuQD"
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for comparison\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove duplicates and clean data\"\"\"\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    for item in data:\n",
    "        # Create a unique key from instruction and output\n",
    "        key = (normalize_text(item[\"instruction\"]), normalize_text(item[\"output\"]))\n",
    "\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            cleaned.append(item)\n",
    "        else:\n",
    "            duplicates_removed += 1\n",
    "\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Clean data: {len(cleaned)} pairs\")\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "cleaned_data = clean_data(combined_data)\n",
    "\n",
    "# Display statistics by source\n",
    "source_counts = Counter([item[\"source\"] for item in cleaned_data])\n",
    "print(\"\\nData by source:\")\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"  {source}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPPOaYf5KuQE"
   },
   "source": [
    "## Step 5: Train/Validation Split (80:20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLfoQC6aKuQE"
   },
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "train_data, val_data = train_test_split(\n",
    "    cleaned_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training data: {len(train_data)} pairs\")\n",
    "print(f\"Validation data: {len(val_data)} pairs\")\n",
    "\n",
    "# Verify split maintains source distribution\n",
    "train_sources = Counter([item[\"source\"] for item in train_data])\n",
    "val_sources = Counter([item[\"source\"] for item in val_data])\n",
    "\n",
    "print(\"\\nTraining data by source:\")\n",
    "for source, count in train_sources.items():\n",
    "    print(f\"  {source}: {count}\")\n",
    "\n",
    "print(\"\\nValidation data by source:\")\n",
    "for source, count in val_sources.items():\n",
    "    print(f\"  {source}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr4xoT0eKuQE"
   },
   "source": [
    "## Step 6: Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdrGgJx3KuQE"
   },
   "outputs": [],
   "source": [
    "def save_jsonl(data, filename):\n",
    "    \"\"\"Save data to JSONL format\"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    # Save file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"✅ Saved {len(data)} items to {filename}\")\n",
    "\n",
    "# Save all datasets with dynamic paths\n",
    "output_dir = f'{base_dir}/data/processed'\n",
    "print(f\"\\nSaving outputs to: {output_dir}/\")\n",
    "\n",
    "save_jsonl(cleaned_data, f'{output_dir}/lawbot_cleaned.jsonl')\n",
    "save_jsonl(train_data, f'{output_dir}/train.jsonl')\n",
    "save_jsonl(val_data, f'{output_dir}/val.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRGQpslIKuQF"
   },
   "source": [
    "## Step 7: Generate Preprocessing Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hVHyGgnKuQF"
   },
   "outputs": [],
   "source": [
    "def generate_report(data, train_data, val_data, filename):\n",
    "    \"\"\"Generate preprocessing report\"\"\"\n",
    "    report = {\n",
    "        \"dataset_statistics\": {\n",
    "            \"total_samples\": len(data),\n",
    "            \"train_samples\": len(train_data),\n",
    "            \"val_samples\": len(val_data),\n",
    "            \"train_ratio\": len(train_data) / len(data),\n",
    "            \"val_ratio\": len(val_data) / len(data)\n",
    "        },\n",
    "        \"source_distribution\": {\n",
    "            \"overall\": dict(Counter([item[\"source\"] for item in data])),\n",
    "            \"train\": dict(Counter([item[\"source\"] for item in train_data])),\n",
    "            \"val\": dict(Counter([item[\"source\"] for item in val_data]))\n",
    "        },\n",
    "        \"text_statistics\": {\n",
    "            \"avg_instruction_length\": sum(len(item[\"instruction\"]) for item in data) / len(data),\n",
    "            \"avg_output_length\": sum(len(item[\"output\"]) for item in data) / len(data),\n",
    "            \"max_instruction_length\": max(len(item[\"instruction\"]) for item in data),\n",
    "            \"max_output_length\": max(len(item[\"output\"]) for item in data)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"Preprocessing Report:\")\n",
    "    print(json.dumps(report, indent=2, ensure_ascii=False))\n",
    "\n",
    "    return report\n",
    "\n",
    "report = generate_report(cleaned_data, train_data, val_data, f'{base_dir}/data/processed/preprocessing_report.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSG0yzelKuQF"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Phase 1 completed successfully! The dataset has been:\n",
    "1. ✅ Loaded from three JSON files\n",
    "2. ✅ Transformed to instruction format\n",
    "3. ✅ Cleaned and deduplicated\n",
    "4. ✅ Split into train/validation sets (80:20)\n",
    "5. ✅ Saved as JSONL files\n",
    "6. ✅ Preprocessing report generated\n",
    "\n",
    "**Deliverables:**\n",
    "- `data/processed/lawbot_cleaned.jsonl` - Complete cleaned dataset\n",
    "- `data/processed/train.jsonl` - Training set\n",
    "- `data/processed/val.jsonl` - Validation set\n",
    "- `data/processed/preprocessing_report.json` - Statistics report\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
