{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Fine-Tuning LawBot with Qwen2.5-1.5B\n",
        "\n",
        "## Objectives:\n",
        "1. Load Qwen2.5-1.5B-Instruct model\n",
        "2. Apply 4-bit QLoRA using Unsloth\n",
        "3. Fine-tune on legal Q&A data\n",
        "4. Evaluate model performance\n",
        "5. Save adapter weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install unsloth for fast fine-tuning\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "\n",
        "# Import libraries\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load train and validation data\n",
        "def load_jsonl(filename):\n",
        "    data = []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_jsonl('../data/processed/train.jsonl')\n",
        "val_data = load_jsonl('../data/processed/val.jsonl')\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Save to temporary JSON files for datasets library\n",
        "with open('/tmp/train.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for item in train_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "with open('/tmp/val.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for item in val_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# Load with datasets library\n",
        "dataset = load_dataset('json', data_files={'train': '/tmp/train.jsonl', 'val': '/tmp/val.jsonl'})\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Qwen2.5-1.5B Model with QLoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model.config.name_or_path}\")\n",
        "print(f\"Max sequence length: {2048}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Dataset Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_instruction(data):\n",
        "    \"\"\"Format data for Qwen2.5 instruction following\"\"\"\n",
        "    instruction = data[\"instruction\"]\n",
        "    output = data[\"output\"]\n",
        "    \n",
        "    text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\\n\"\n",
        "    return text\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
        "print(\"Sample formatted text:\")\n",
        "print(dataset[\"train\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fine-Tuning Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"../models/adapters\",\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        save_total_limit=3,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"../models/adapters/lawbot_qwen_adapter\")\n",
        "tokenizer.save_pretrained(\"../models/adapters/lawbot_qwen_adapter\")\n",
        "\n",
        "print(\"Adapter weights saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu import BLEU\n",
        "import json\n",
        "\n",
        "# Load evaluation metrics\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "bleu_scorer = BLEU()\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, num_samples=10):\n",
        "    \"\"\"Evaluate model on sample data\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    FastLanguageModel.for_inference(model)\n",
        "    \n",
        "    for i, sample in enumerate(dataset[:num_samples]):\n",
        "        prompt = f\"<|im_start|>user\\n{sample['instruction']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        \n",
        "        outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "        \n",
        "        # Extract generated text\n",
        "        generated_text = generated.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n",
        "        ground_truth = sample['output']\n",
        "        \n",
        "        # Calculate ROUGE scores\n",
        "        rouge_scores = scorer.score(ground_truth, generated_text)\n",
        "        \n",
        "        # Calculate BLEU score\n",
        "        bleu_score = bleu_scorer.sentence_score(generated_text, [ground_truth])\n",
        "        \n",
        "        results.append({\n",
        "            'instruction': sample['instruction'][:100],\n",
        "            'generated': generated_text[:200],\n",
        "            'ground_truth': ground_truth[:200],\n",
        "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
        "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
        "            'bleu': bleu_score.score / 100.0\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = evaluate_model(model, tokenizer, val_data, num_samples=20)\n",
        "\n",
        "# Calculate average scores\n",
        "avg_rouge1 = sum(r['rouge1'] for r in eval_results) / len(eval_results)\n",
        "avg_rouge2 = sum(r['rouge2'] for r in eval_results) / len(eval_results)\n",
        "avg_rougeL = sum(r['rougeL'] for r in eval_results) / len(eval_results)\n",
        "avg_bleu = sum(r['bleu'] for r in eval_results) / len(eval_results)\n",
        "\n",
        "print(f\"\\nEvaluation Results:\")\n",
        "print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
        "print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
        "\n",
        "# Save evaluation results\n",
        "with open('../data/processed/evaluation_results.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'avg_scores': {\n",
        "            'rouge1': avg_rouge1,\n",
        "            'rouge2': avg_rouge2,\n",
        "            'rougeL': avg_rougeL,\n",
        "            'bleu': avg_bleu\n",
        "        },\n",
        "        'detailed_results': eval_results[:5]  # Save first 5 for review\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\nEvaluation results saved to data/processed/evaluation_results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Phase 2 completed successfully! The model has been:\n",
        "1. ✅ Loaded Qwen2.5-1.5B-Instruct model\n",
        "2. ✅ Applied QLoRA with 4-bit quantization\n",
        "3. ✅ Fine-tuned on legal Q&A data (3 epochs)\n",
        "4. ✅ Evaluated with ROUGE and BLEU metrics\n",
        "5. ✅ Saved adapter weights\n",
        "\n",
        "**Deliverables:**\n",
        "- `models/adapters/lawbot_qwen_adapter/` - Fine-tuned adapter weights\n",
        "- `data/processed/evaluation_results.json` - Performance metrics\n",
        "- Training history with validation loss tracking\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
